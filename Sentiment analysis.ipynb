{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Example of usage of Distilbert sentiment analysis model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998623132705688}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model = \"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "text = \"This product works amazingly well!\"\n",
    "result = classifier(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Importing the libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import logging\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Logger***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    def __init__(self, model_name, batch_size = 128, max_length = 128):\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        #Load model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        if self.device == \"cuda\":\n",
    "            self.model = self.model.half()\n",
    "        \n",
    "        self.model.eval()\n",
    "\n",
    "    def _batch_tokenize(self, texts: List[str]):\n",
    "        return self.tokenizer(texts, max_length = self.max_length, padding = True, truncation = True, return_tensors = \"pt\")\n",
    "\n",
    "    def process_reviews(self, df, output_file, chunk_size):\n",
    "        total_rows = len(df)\n",
    "        results = []\n",
    "        \n",
    "        #Process in chunks\n",
    "        for start_idx in tqdm(range(0, total_rows, chunk_size)):\n",
    "            end_idx = min(start_idx + chunk_size, total_rows)\n",
    "            chunk_df = df.iloc[start_idx : end_idx]\n",
    "            \n",
    "            #Process each batch within the chunk\n",
    "            for batch_start in range(0, len(chunk_df), self.batch_size):\n",
    "                batch_end = min(batch_start + self.batch_size, len(chunk_df))\n",
    "                batch_df = chunk_df.iloc[batch_start : batch_end]\n",
    "\n",
    "                #Here, we are retreiving text and review\"s id to put them later on file\n",
    "                texts = batch_df[\"text\"].tolist()\n",
    "                review_ids = batch_df.index.tolist()\n",
    "                \n",
    "                #Tokenize\n",
    "                encoded = self._batch_tokenize(texts)\n",
    "                input_ids = encoded[\"input_ids\"].to(self.device)  # Keep as LongTensor\n",
    "                attention_mask = encoded[\"attention_mask\"].to(self.device)\n",
    "                \n",
    "                #Get predictions\n",
    "                with torch.no_grad():\n",
    "                    if self.device == \"cuda\":\n",
    "                        #Only convert attention_mask to half precision!\n",
    "                        attention_mask = attention_mask.half()\n",
    "\n",
    "                    outputs = self.model(input_ids, attention_mask = attention_mask)\n",
    "                    probs = torch.nn.functional.softmax(outputs.logits, dim = -1)\n",
    "                    batch_preds = (probs[:, 1] > 0.5).cpu().numpy()\n",
    "                    batch_confs = probs.max(dim = 1)[0].cpu().numpy()\n",
    "                \n",
    "                #Store results\n",
    "                batch_results = pd.DataFrame({\n",
    "                    \"review_id\" : review_ids,\n",
    "                    \"sentiment\" : [\"positive\" if pred else \"negative\" for pred in batch_preds],\n",
    "                    \"confidence\" : batch_confs\n",
    "                })\n",
    "                results.append(batch_results)\n",
    "                \n",
    "                #Clear GPU memory\n",
    "                if self.device == \"cuda\":\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            #Save chunk results\n",
    "            pd.concat(results).to_csv(output_file, mode = \"a\", header = not bool(start_idx), index = False)\n",
    "            results = []\n",
    "            gc.collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Connect to MongoDB and load the reviews***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KU_O5udG6zpxOg-VcAEodg</th>\n",
       "      <td>675167d1b78f0be484408abb</td>\n",
       "      <td>If you decide to eat here, just be aware it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BiTunyQ73aT9WBnpR9DZGw</th>\n",
       "      <td>675167d1b78f0be484408abc</td>\n",
       "      <td>I've taken a lot of spin classes over the year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saUsX_uimxRlCVr67Z4Jig</th>\n",
       "      <td>675167d1b78f0be484408abd</td>\n",
       "      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AqPFMleE6RsU23_auESxiA</th>\n",
       "      <td>675167d1b78f0be484408abe</td>\n",
       "      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sx8TMOWLNuJBWer-0pcmoA</th>\n",
       "      <td>675167d1b78f0be484408abf</td>\n",
       "      <td>Cute interior and owner (?) gave us tour of up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H0RIamZu0B0Ei0P4aeh3sQ</th>\n",
       "      <td>67516883b78f0be484ab345d</td>\n",
       "      <td>Latest addition to services from ICCU is Apple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shTPgbgdwTHSuU67mGCmZQ</th>\n",
       "      <td>67516883b78f0be484ab345e</td>\n",
       "      <td>This spot offers a great, affordable east week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YNfNhgZlaaCO5Q_YJR4rEw</th>\n",
       "      <td>67516883b78f0be484ab345f</td>\n",
       "      <td>This Home Depot won me over when I needed to g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i-I4ZOhoX70Nw5H0FwrQUA</th>\n",
       "      <td>67516883b78f0be484ab3460</td>\n",
       "      <td>For when I'm feeling like ignoring my calorie-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RwcKOdEuLRHNJe4M9-qpqg</th>\n",
       "      <td>67516883b78f0be484ab3461</td>\n",
       "      <td>Located in the 'Walking District' in Nashville...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6990247 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             _id  \\\n",
       "review_id                                          \n",
       "KU_O5udG6zpxOg-VcAEodg  675167d1b78f0be484408abb   \n",
       "BiTunyQ73aT9WBnpR9DZGw  675167d1b78f0be484408abc   \n",
       "saUsX_uimxRlCVr67Z4Jig  675167d1b78f0be484408abd   \n",
       "AqPFMleE6RsU23_auESxiA  675167d1b78f0be484408abe   \n",
       "Sx8TMOWLNuJBWer-0pcmoA  675167d1b78f0be484408abf   \n",
       "...                                          ...   \n",
       "H0RIamZu0B0Ei0P4aeh3sQ  67516883b78f0be484ab345d   \n",
       "shTPgbgdwTHSuU67mGCmZQ  67516883b78f0be484ab345e   \n",
       "YNfNhgZlaaCO5Q_YJR4rEw  67516883b78f0be484ab345f   \n",
       "i-I4ZOhoX70Nw5H0FwrQUA  67516883b78f0be484ab3460   \n",
       "RwcKOdEuLRHNJe4M9-qpqg  67516883b78f0be484ab3461   \n",
       "\n",
       "                                                                     text  \n",
       "review_id                                                                  \n",
       "KU_O5udG6zpxOg-VcAEodg  If you decide to eat here, just be aware it is...  \n",
       "BiTunyQ73aT9WBnpR9DZGw  I've taken a lot of spin classes over the year...  \n",
       "saUsX_uimxRlCVr67Z4Jig  Family diner. Had the buffet. Eclectic assortm...  \n",
       "AqPFMleE6RsU23_auESxiA  Wow!  Yummy, different,  delicious.   Our favo...  \n",
       "Sx8TMOWLNuJBWer-0pcmoA  Cute interior and owner (?) gave us tour of up...  \n",
       "...                                                                   ...  \n",
       "H0RIamZu0B0Ei0P4aeh3sQ  Latest addition to services from ICCU is Apple...  \n",
       "shTPgbgdwTHSuU67mGCmZQ  This spot offers a great, affordable east week...  \n",
       "YNfNhgZlaaCO5Q_YJR4rEw  This Home Depot won me over when I needed to g...  \n",
       "i-I4ZOhoX70Nw5H0FwrQUA  For when I'm feeling like ignoring my calorie-...  \n",
       "RwcKOdEuLRHNJe4M9-qpqg  Located in the 'Walking District' in Nashville...  \n",
       "\n",
       "[6990247 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"yelp\"]\n",
    "\n",
    "reviews = db[\"reviews\"].find({}, {\n",
    "    \"review_id\" : 1,\n",
    "    \"text\" : 1\n",
    "})\n",
    "\n",
    "reviews = pd.DataFrame(reviews)\n",
    "reviews = reviews.set_index(\"review_id\")\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Make predictions using the model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using device: cuda\n",
      "  0%|          | 0/700 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentAnalyzer(\"distilbert-base-uncased-finetuned-sst-2-english\", batch_size = 512, max_length = 512)\n",
    "\n",
    "analyzer.process_reviews(reviews, \"sentiment_results_with_neutral.csv\", 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to retreive the sentiments from file, you need to execute this cell\n",
    "sentiment = pd.read_csv(\"sentiment_results.csv\")\n",
    "sentiment = sentiment.set_index(\"review_id\")\n",
    "sentiment_dict = sentiment.to_dict(orient = \"index\")\n",
    "sentiment_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Loading the results into MongoDB (```reviews``` collection)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll use batches to boost updating process\n",
    "batch_size = 10000\n",
    "\n",
    "sentiment_list = list(sentiment_dict.items())\n",
    "num_batches = math.ceil(len(sentiment_list) / batch_size)\n",
    "\n",
    "with tqdm(total = num_batches) as pbar:\n",
    "\n",
    "    for i in range(0, len(sentiment_list), batch_size):\n",
    "        batch_items = sentiment_list[i : i + batch_size]\n",
    "        \n",
    "        #To update efficently all the documents, we'll use \"bulk_write\" that is able to minimize the number of db operations\n",
    "        db[\"reviews\"].bulk_write([\n",
    "                                    pymongo.UpdateOne({\n",
    "                                                          \"review_id\" : review_id\n",
    "                                                      },\n",
    "                                                      {\n",
    "                                                          \"$set\" : data\n",
    "                                                      })\n",
    "                                                      for review_id, data in batch_items\n",
    "                                ])\n",
    "        \n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Loading the results into MongoDB (```businesses_merged``` collection)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_to_business_map = db[\"reviews\"].find({},\n",
    "                                            {   \n",
    "                                                \"_id\" : 0,\n",
    "                                                \"review_id\" : 1,\n",
    "                                                \"business_id\" : 1\n",
    "                                            })\n",
    "\n",
    "review_to_business_map = pd.DataFrame(review_to_business_map).set_index(\"review_id\")[\"business_id\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To speed up update process, we'll create atemporary index on \"reviews.review_id\". It is unique and \"sparse\" because some documents might not have \"reviews\" subcollection\n",
    "db[\"businesses_merged\"].create_index([\"reviews.review_id\"], unique = True, sparse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2500\n",
    "\n",
    "sentiment_list = list(sentiment_dict.items())\n",
    "num_batches = math.ceil(len(sentiment_list) / batch_size)\n",
    "\n",
    "with tqdm(total = num_batches) as pbar:\n",
    "\n",
    "    for i in range(0, len(sentiment_list), batch_size):\n",
    "        batch_items = sentiment_list[i : i + batch_size]\n",
    "        \n",
    "        #To update efficently all the documents, we'll use \"bulk_write\" that is able to minimize the number of db operations\n",
    "        db[\"businesses_merged\"].bulk_write([\n",
    "                                                  pymongo.UpdateOne({\n",
    "                                                                        #First finnd the document using \"business_id\", then find the element in the embedded array using \"review_id\"  \n",
    "                                                                        \"business_id\" : review_to_business_map[review_id],\n",
    "                                                                        \"reviews.review_id\" : review_id \n",
    "                                                                    },\n",
    "                                                                    {\n",
    "                                                                        \"$set\" : {\n",
    "                                                                            #$ is used to update only the matched document(s)\n",
    "                                                                            \"reviews.$.sentiment\" : data[\"sentiment\"],\n",
    "                                                                            \"reviews.$.confidence\" : data[\"confidence\"]  \n",
    "                                                                        }\n",
    "                                                                    })\n",
    "                                                                    for review_id, data in batch_items\n",
    "                                          ])\n",
    "        \n",
    "        pbar.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
