{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Example of usage of Distilbert sentiment analysis model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998623132705688}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model = \"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "text = \"This product works amazingly well!\"\n",
    "result = classifier(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Importing the libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import logging\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Logger***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Model usage***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    def __init__(self, model_name, batch_size = 128, max_length = 128):\n",
    "        \"\"\"\n",
    "        Initialize the sentiment analyzer with a pre-trained model\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name/path of the pre-trained model to load\n",
    "            batch_size (int): Number of texts to process at once\n",
    "            max_length (int): Maximum length of input sequences\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        #Determine if GPU is available, otherwise use CPU\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        #Load pre-trained model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.to(self.device)  # Move model to GPU if available\n",
    "\n",
    "        #Convert model to half precision if using GPU to reduce memory usage\n",
    "        if self.device == \"cuda\":\n",
    "            self.model = self.model.half()\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "    def _batch_tokenize(self, texts: List[str]):\n",
    "        \"\"\"\n",
    "        Tokenize a batch of texts\n",
    "        \n",
    "        Args:\n",
    "            texts (List[str]): List of input texts to tokenize\n",
    "            \n",
    "        Returns:\n",
    "            dict: Tokenized inputs including input_ids and attention_mask\n",
    "        \"\"\"\n",
    "        return self.tokenizer(texts, \n",
    "                            max_length=self.max_length, \n",
    "                            padding=True, \n",
    "                            truncation=True, \n",
    "                            return_tensors=\"pt\")\n",
    "\n",
    "    def process_reviews(self, df, output_file, chunk_size):\n",
    "        \"\"\"\n",
    "        Process reviews in chunks and batches to handle large datasets efficiently\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing reviews to analyze\n",
    "            output_file (str): Path to save results\n",
    "            chunk_size (int): Number of reviews to process before saving to disk\n",
    "        \"\"\"\n",
    "        total_rows = len(df)\n",
    "        results = []\n",
    "        \n",
    "        #Process DataFrame in chunks to manage memory\n",
    "        for start_idx in tqdm(range(0, total_rows, chunk_size)):\n",
    "            end_idx = min(start_idx + chunk_size, total_rows)\n",
    "            chunk_df = df.iloc[start_idx : end_idx]\n",
    "            \n",
    "            #Process each chunk in smaller batches\n",
    "            for batch_start in range(0, len(chunk_df), self.batch_size):\n",
    "                batch_end = min(batch_start + self.batch_size, len(chunk_df))\n",
    "                batch_df = chunk_df.iloc[batch_start : batch_end]\n",
    "\n",
    "                #Extract text and IDs from batch\n",
    "                texts = batch_df[\"text\"].tolist()\n",
    "                review_ids = batch_df.index.tolist()\n",
    "                \n",
    "                #Tokenize the batch\n",
    "                encoded = self._batch_tokenize(texts)\n",
    "                input_ids = encoded[\"input_ids\"].to(self.device)\n",
    "                attention_mask = encoded[\"attention_mask\"].to(self.device)\n",
    "                \n",
    "                #Generate predictions\n",
    "                with torch.no_grad():  # Disable gradient calculation for inference\n",
    "                    if self.device == \"cuda\":\n",
    "                        # onvert only attention mask to half precision\n",
    "                        attention_mask = attention_mask.half()\n",
    "\n",
    "                    #Get model outputs and convert to probabilities\n",
    "                    outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "                    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                    # Get binary predictions (positive/negative) and confidence scores\n",
    "                    batch_preds = (probs[:, 1] > 0.5).cpu().numpy()\n",
    "                    batch_confs = probs.max(dim=1)[0].cpu().numpy()\n",
    "                \n",
    "                #Store batch results in DataFrame\n",
    "                batch_results = pd.DataFrame({\n",
    "                    \"review_id\": review_ids,\n",
    "                    \"sentiment\": [\"positive\" if pred else \"negative\" for pred in batch_preds],\n",
    "                    \"confidence\": batch_confs\n",
    "                })\n",
    "                results.append(batch_results)\n",
    "                \n",
    "                #Free up GPU memory after each batch\n",
    "                if self.device == \"cuda\":\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            #Save results for current chunk and clear memory\n",
    "            pd.concat(results).to_csv(output_file, mode=\"a\", header=not bool(start_idx), index=False)\n",
    "            results = []\n",
    "\n",
    "            #Run garbage collection to free memory\n",
    "            gc.collect()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Connect to MongoDB and load the reviews***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KU_O5udG6zpxOg-VcAEodg</th>\n",
       "      <td>If you decide to eat here, just be aware it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BiTunyQ73aT9WBnpR9DZGw</th>\n",
       "      <td>I've taken a lot of spin classes over the year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saUsX_uimxRlCVr67Z4Jig</th>\n",
       "      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AqPFMleE6RsU23_auESxiA</th>\n",
       "      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sx8TMOWLNuJBWer-0pcmoA</th>\n",
       "      <td>Cute interior and owner (?) gave us tour of up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H0RIamZu0B0Ei0P4aeh3sQ</th>\n",
       "      <td>Latest addition to services from ICCU is Apple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shTPgbgdwTHSuU67mGCmZQ</th>\n",
       "      <td>This spot offers a great, affordable east week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YNfNhgZlaaCO5Q_YJR4rEw</th>\n",
       "      <td>This Home Depot won me over when I needed to g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i-I4ZOhoX70Nw5H0FwrQUA</th>\n",
       "      <td>For when I'm feeling like ignoring my calorie-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RwcKOdEuLRHNJe4M9-qpqg</th>\n",
       "      <td>Located in the 'Walking District' in Nashville...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6990247 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     text\n",
       "review_id                                                                \n",
       "KU_O5udG6zpxOg-VcAEodg  If you decide to eat here, just be aware it is...\n",
       "BiTunyQ73aT9WBnpR9DZGw  I've taken a lot of spin classes over the year...\n",
       "saUsX_uimxRlCVr67Z4Jig  Family diner. Had the buffet. Eclectic assortm...\n",
       "AqPFMleE6RsU23_auESxiA  Wow!  Yummy, different,  delicious.   Our favo...\n",
       "Sx8TMOWLNuJBWer-0pcmoA  Cute interior and owner (?) gave us tour of up...\n",
       "...                                                                   ...\n",
       "H0RIamZu0B0Ei0P4aeh3sQ  Latest addition to services from ICCU is Apple...\n",
       "shTPgbgdwTHSuU67mGCmZQ  This spot offers a great, affordable east week...\n",
       "YNfNhgZlaaCO5Q_YJR4rEw  This Home Depot won me over when I needed to g...\n",
       "i-I4ZOhoX70Nw5H0FwrQUA  For when I'm feeling like ignoring my calorie-...\n",
       "RwcKOdEuLRHNJe4M9-qpqg  Located in the 'Walking District' in Nashville...\n",
       "\n",
       "[6990247 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"yelp\"]\n",
    "\n",
    "reviews = db[\"reviews\"].find({}, {\n",
    "    \"_id\" : 0,\n",
    "    \"review_id\" : 1,\n",
    "    \"text\" : 1\n",
    "})\n",
    "\n",
    "reviews = pd.DataFrame(reviews)\n",
    "reviews = reviews.set_index(\"review_id\")\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Make predictions using the model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using device: cuda\n",
      "100%|██████████| 700/700 [18:54:25<00:00, 97.24s/it]   \n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentAnalyzer(\"distilbert-base-uncased-finetuned-sst-2-english\", batch_size = 512, max_length = 128)\n",
    "\n",
    "analyzer.process_reviews(reviews, \"sentiment_results_v2.csv\", 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to retreive the sentiments from file, you need to execute this cell\n",
    "sentiment = pd.read_csv(\"sentiment_results.csv\")\n",
    "sentiment = sentiment.set_index(\"review_id\")\n",
    "sentiment_dict = sentiment.to_dict(orient = \"index\")\n",
    "sentiment_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Loading the results into MongoDB (```reviews``` collection)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll use batches to boost updating process\n",
    "batch_size = 10000\n",
    "\n",
    "sentiment_list = list(sentiment_dict.items())\n",
    "num_batches = math.ceil(len(sentiment_list) / batch_size)\n",
    "\n",
    "with tqdm(total = num_batches) as pbar:\n",
    "\n",
    "    for i in range(0, len(sentiment_list), batch_size):\n",
    "        batch_items = sentiment_list[i : i + batch_size]\n",
    "        \n",
    "        #To update efficently all the documents, we'll use \"bulk_write\" that is able to minimize the number of db operations\n",
    "        db[\"reviews\"].bulk_write([\n",
    "                                    pymongo.UpdateOne({\n",
    "                                                          \"review_id\" : review_id\n",
    "                                                      },\n",
    "                                                      {\n",
    "                                                          \"$set\" : data\n",
    "                                                      })\n",
    "                                                      for review_id, data in batch_items\n",
    "                                ])\n",
    "        \n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Loading the results into MongoDB (```businesses_merged``` collection)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_to_business_map = db[\"reviews\"].find({},\n",
    "                                            {   \n",
    "                                                \"_id\" : 0,\n",
    "                                                \"review_id\" : 1,\n",
    "                                                \"business_id\" : 1\n",
    "                                            })\n",
    "\n",
    "review_to_business_map = pd.DataFrame(review_to_business_map).set_index(\"review_id\")[\"business_id\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To speed up update process, we'll create atemporary index on \"reviews.review_id\". It is unique and \"sparse\" because some documents might not have \"reviews\" subcollection\n",
    "db[\"businesses_merged\"].create_index([\"reviews.review_id\"], unique = True, sparse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2500\n",
    "\n",
    "sentiment_list = list(sentiment_dict.items())\n",
    "num_batches = math.ceil(len(sentiment_list) / batch_size)\n",
    "\n",
    "with tqdm(total = num_batches) as pbar:\n",
    "\n",
    "    for i in range(0, len(sentiment_list), batch_size):\n",
    "        batch_items = sentiment_list[i : i + batch_size]\n",
    "        \n",
    "        #To update efficently all the documents, we'll use \"bulk_write\" that is able to minimize the number of db operations\n",
    "        db[\"businesses_merged\"].bulk_write([\n",
    "                                                  pymongo.UpdateOne({\n",
    "                                                                        #First finnd the document using \"business_id\", then find the element in the embedded array using \"review_id\"  \n",
    "                                                                        \"business_id\" : review_to_business_map[review_id],\n",
    "                                                                        \"reviews.review_id\" : review_id \n",
    "                                                                    },\n",
    "                                                                    {\n",
    "                                                                        \"$set\" : {\n",
    "                                                                            #$ is used to update only the matched document(s)\n",
    "                                                                            \"reviews.$.sentiment\" : data[\"sentiment\"],\n",
    "                                                                            \"reviews.$.confidence\" : data[\"confidence\"]  \n",
    "                                                                        }\n",
    "                                                                    })\n",
    "                                                                    for review_id, data in batch_items\n",
    "                                          ])\n",
    "        \n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ***Loading the results into MongoDB (```users_merged``` collection)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_to_user_map = db[\"reviews\"].find({},\n",
    "                                            {   \n",
    "                                                \"_id\" : 0,\n",
    "                                                \"review_id\" : 1,\n",
    "                                                \"user_id\" : 1\n",
    "                                            })\n",
    "\n",
    "review_to_user_map = pd.DataFrame(review_to_user_map).set_index(\"review_id\")[\"user_id\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To speed up update process, we'll create atemporary index on \"reviews.review_id\". It is unique and \"sparse\" because some documents might not have \"reviews\" subcollection\n",
    "db[\"users_merged\"].create_index([\"reviews.review_id\"], unique = True, sparse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2500\n",
    "\n",
    "sentiment_list = list(sentiment_dict.items())\n",
    "num_batches = math.ceil(len(sentiment_list) / batch_size)\n",
    "\n",
    "with tqdm(total = num_batches) as pbar:\n",
    "\n",
    "    for i in range(0, len(sentiment_list), batch_size):\n",
    "        batch_items = sentiment_list[i : i + batch_size]\n",
    "        \n",
    "        #To update efficently all the documents, we'll use \"bulk_write\" that is able to minimize the number of db operations\n",
    "        db[\"users_merged\"].bulk_write([\n",
    "                                                  pymongo.UpdateOne({\n",
    "                                                                        #First finnd the document using \"user_id\", then find the element in the embedded array using \"review_id\"  \n",
    "                                                                        \"user_id\" : review_to_user_map[review_id],\n",
    "                                                                        \"reviews.review_id\" : review_id \n",
    "                                                                    },\n",
    "                                                                    {\n",
    "                                                                        \"$set\" : {\n",
    "                                                                            #$ is used to update only the matched document(s)\n",
    "                                                                            \"reviews.$.sentiment\" : data[\"sentiment\"],\n",
    "                                                                            \"reviews.$.confidence\" : data[\"confidence\"]  \n",
    "                                                                        }\n",
    "                                                                    })\n",
    "                                                                    for review_id, data in batch_items\n",
    "                                          ])\n",
    "        \n",
    "        pbar.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
